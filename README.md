# Singular Value Decomposition and the Empirical Spectral Distribution
## SVD visualisation

> In this section, we will be working in `./svd_visualization.py` and `./coords_and_svd.py`. We are also using zero-based indexing, because I am a programmer.

SVD is a tool which allows you to find which dimensions in a dataset have a lot of variance. What does that actually mean? Let's take the following set of 200 points in 3D space;

![The bare, unanalysed space](./gifs/bare-space.gif)

Our data initially exists without any obvious coordinate system applied to it, and it's spinning through space. Or perhaps we're spinning around it; the two are identical from our perspective. The goal is impose a coordinate system onto this data in which the zeroth coordinate has the most variance, the next coordinate has the next most, et cetera. We can see that in our case, the data is pancake shaped, with more variance in one direction than others. Let's take that direction as our zeroth 'basis vector', that is the zeroth coordinate in the system that we are constructing.

![The zeroth basis vector has been applied](./gifs/zeroth-basis-applied.gif)

What does this choice of basis vector mean in practise? It is chosen so that, if we were to pick one of our points in the data, simply knowing the closest point on this 1D line would give the best guess on where that point lies in the wider space. If we were to draw this line in a different direction, and there are infinitely many to choose from, we would have a worse guess. 

But even our mathematically optimal zeroth basis vector still can't yield a great guess, so how do we go about selecting another? Here, we can take advantage of a simple fact, our next basis vector has to be perpendicular to the zeroth. If it wasn't, there would be redundant information between the two basis vectors. This also arises from the fact that SVD computes a rotation, but that's a rabbit hole we can leave until later.

In 3D space, we still have an entire plane of vectors to choose from for our second basis vector.

![We have a plane of possibilities for the first basis vector](./gifs/first-basis-possibilities.gif)

We can, however, repeat a version of the same trick we applied to find the zeroth basis vector. We find the line which, knowing where the point sits on the plane defined by the zeroth and first basis vectors, tells us the most about where the point is in 3D space.

![The first basis vector has been applied](./gifs/first-basis-applied.gif)

Where do we go from here? Well, we are lucky enough to be living in a 3D space. In 3D space, given two vectors, there is only one vector perpendicular to both of them. Well, technically there are two (up and down) but they would simply yield an inverted sign. Let's apply that last basis vector now.

![The second basis vector has been applied](./gifs/second-basis-applied.gif)

Because we have a 3D input space, and a 3D output space, knowing the closest point on all of these three basis vectors tells us where the data is in 3D space. SVD is sometimes used for a type of lossy compression known as dimensional reduction. If you were to flatten all the points so that you only knew their position relative to the zeroth and first basis vector, you could still have a pretty good guess of where the point is in 3D space while only having to communicate two values. 

You can also now see how SVD computes a rotation, essentially constructing a complete coordinate system for our data. 

I have given myself some artistic license in this visualisation; the basis vectors are of different lengths. The basis vectors produced by SVD are always unit (1) length because SVD creates a rotation matrix without any scaling. To produce this visualisation, I have scaled each basis by a set of another set of outputs of the SVD algorithm, the `S` values. These are technically known as the singular values, but you can think of them as 'significance'. You can see that the zeroth is the highest significance, the first the next highest, and the final the least. If we had 100-Dimensional data, it would produce 100 perpendicular basis vectors with 100 significance levels, starting with the most and ending with the least significant.

> SVD is typically computed all at once rather than basis by basis, but this model is very useful in thinking about what exactly SVD is doing.

## Empirical Spectral Distribution

> In this section, we will be working in `./empirical_spectral_distribution.py` and `./coords_and_svd.py`.

Vector spaces become very hard to visualise past 3 dimensions, but SVD remains useful. Let's focus on one type of analysis which can be performed, taking the Empirical Spectral Distribution (ESD).

When we use numpy to compute the SVD in `./coords_and_svd.py`, we get three cryptically named outputs;

```
U, S, Vh = np.linalg.svd(coords_world)
```

In reverse order:
 - `Vh` is the rotation matrix which maps whatever input space we used to the set of basis vectors we constructed above, where they are ranked from most to least variance. For the same set of input vectors, we can apply any rotation we want, and SVD is generally guaranteed to converge the same output space. This is rotation matrix is then applied in the next line;

```
coords_world = np.matmul(coords_world, Vh)
```

- `S` contains the singular values (I call them 'significance' above). It's how long we portrayed our basis vectors to be in the last visualisation. For each of the basis vectors in `Vh`, it tells us how important they are to describing the variance of the data.
- Finally, `U` represents, for each of our input data points, what combination of our constructed basis vectors can reconstruct them. Unlike `Vh` and `S`, this part of the analysis is not aggregated and gives one output per input vector.

ESD analysis only considers the values of `S`, which this is a flat 1-dimensional vector associating significance levels to each of the basis vectors. Let's take those scaled vectors and lay them flat, horizontal, and starting from the same X coordinate.

![vectors laid flat](./imgs/esd-hist-False-large-False.png)

Now let's use that to build a histogram.

![vectors laid flat with histogram](./imgs/esd-hist-True-large-False.png)

Et voila, we can see the worlds most pointless histogram. Because we only have three basis vectors to analyse, we have more buckets than we have values to fill them. Let's do something more interesting which shows the power of ESD to analyse very large vector spaces. Instead of working in a 3D space, let's instead work in a 1000D space, and then take 1000 vectors to analyse. The values in each row and column of this input matrix are all sampled from the same normal distribution. This means that they are statistically identical. They are also uncorrelated, meaning they are independent. This is a special type of matrix known as an independent and identically distributed (IID) matrix. When we look at distribution of the SVD singular values, we see;

![large-dimensional-histogram](./imgs/esd-hist-True-large-True-dist-norm.png)

And here is the magic of the ESD; we can take a vector space far too large to be reasoned about visually, but still make meaningful conclusions about the structure of the underlying space. There is clearly a pattern here, and we can treat the singular values as a distribution and use all of the tools of statistics to analyse what's going on.

ESD analysis is usually introduced towards the beginning of a course on Random Matrix Theory (RMT), surrounded by derivations and notation. While those are vital, there also is a real risk of leaving such a course without any intuitive sense of what's actually going on inside of these operations and spaces. In particular, RMT has led to theories describing the internal structure of a neural network with tools like [WeightWatcher](https://github.com/CalculatedContent/WeightWatcher). Neural networks are composed mostly of fancy matrix multiplications after all, and ESD analysis has shown an unreasonable level of generality in understanding how they work. It can do this without looking at training data or even knowing their intended purpose. Incidentally, my cookies have not recovered since discovering WeightWatcher.

I have a confession to make; I have shown you the most boring type of ESD that exists. We've used a mathematical tool designed to measure correlation and structure to look at a matrix without any of either. The ESD distribution that we saw above in the uncorrelated and normally distributed case is called the Marchenkoâ€“Pastur (MP) law. Let's multiply a series of these matrices together and see what happens to the histogram of its singular values.

![heavy-tailed-histogram](./imgs/esd-hist-True-large-True-dist-heavy.png)

This is the mythical Heavy-Tailed matrix. In layman's terms, the fluctuations in structure in each individual matrix has been amplified through the repeated multiplications and so we have a few very strong singular vectors, and many weak ones. The distribution of singular values here follow a power law, which can be seen from all of those singular values hanging out on their own at the very high end. When a neural network is training well, your singular values tend to look a little like this. The specific type and shape of the power law can tell you a lot about how the layers of your networks are training. It's a bit smushed, though, let's view it with a log y-axis;

![heavy-tailed-histogram-log](./imgs/esd-hist-True-large-True-dist-heavy-log.png)

As we can see it's not a perfect match for a log scale which should be a straight line, but it's not far off especially in the middle bit.

### Further Reading:

 - [Traditional and Heavy-Tailed Self Regularization in Neural Network Models](https://arxiv.org/abs/1901.08276) (Martin & Mahoney, 2019)
 - [WeightWatcher](https://github.com/CalculatedContent/WeightWatcher) on GitHub
